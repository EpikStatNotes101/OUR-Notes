# Hypothesis Testing 

^69f1d3

## Intro 
Suppose we have a random sample from an infinite or a finite population and we try to draw inference regarding the population from such a sample.  Suppose the form of the distribution of the population is $F_\theta$ which is assumed to be known but the parameter $\theta$ is unknown. In many practical problems, we are interested in testing the validity of an assertion about the unknown parameter $\theta$. Some hypothesis is made regarding the parameters and it is tested wheather it is acceptable in the light of the sample observations. For example, suppose we're interested in introducing a high yielding rice variety. We have at our disposal a standard variety having an average yield of $x$ quintal per acre. We want to know whether the average yield of the new variety is higher than $x$. A problem of this type is usually referred to as a problem of testing of hypothesis. 

## Definitions 
Let $\rho = \left\{p(x)\right\}$ be a class of all p.m.f or p.d.f. In testing problem $p(x)$ is unknown, but $\rho$ is known. Our objective is to provide more information about $p(x)$ on the basis of $X = x$. That is to know whether $p(x)\in\rho^*\subset\rho$.

>  **Definition 1:** A hypothesis is a conjecture or assertion about $p(x)$. It is of two types, null hypothesis $(H_0)$ and alternative hypothesis $(H_1)$.

**Null Hypothesis $\mathbf{H_0}$:** 
A hypothesis that is tentatively set up is called null hypothesis. ^d677e1

**Alternative Hypothesis $\mathbf{H_1}$:**
Alternative to $H_0$ is called alternative hypothesis.

We also write $$\left.\begin{matrix}
H_0 : p(x) \in \rho_{H_0} \subset \rho \\ H_1 : p(x) \in \rho_{H_1} \subset \rho
\end{matrix}\right\} \rho_{H_0} \cap \rho_{H_1} = \phi \text{ and } \rho_{H_0} \cup \rho_{H_1} \subseteq \rho$$ 

**Labeling of the distribution** 
Write $\rho = \left\{p(x) = p(x| \theta), \theta \in \Theta \right\}$. Then $'\theta'$ is called the labelling parameter of the distribution and $'\Theta'$ is called the parameter space. 

*Example 1 * 
$X\sim\text{bin}(m,p) \Leftrightarrow X_1, X_2, ...X_m \overset{\text{i.i.d}}{\sim} \text{Bernoulli}(p)$
$\Rightarrow X = \sum_{i=1}^m X_i \sim \text{bin}(m,p)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[[2. Discrete Distributions#^29278e|(binomial distribution)]]
$m$ is known, $\theta = p, \Theta = \left[0,1\right]$, outcome space $\mathfrak{x}= \left\{0,1,2,...m\right\}\equiv\left\{0,1\right\}X\left\{0,1\right\}X...X\left\{0,1\right\}$
$$p(x|\theta)=\binom{m}{x}p^x(1-p)^{m-x} \text{ or } p(\tilde{x}|	\theta)=p^{\sum_{i=1}^{m}x_i}(1-p)^{m-\sum_{i=1}^{m}x_i}$$
$$\rho=\left\{\binom{m}{x}p^{x}{(1-p)}^{m-x}, p \in \left[0, 1\right]\right\}$$ is known but $$\binom{m}{x}p^{x}{(1-p)}^{m-x}$$ is unknown if $p$ is unknown.

**Parametric set up**
$p(x)=p(x|\theta); \theta \in \Theta$. Then we can find $\Theta_{H_0}(\subset\Theta)$ and $\Theta_{H_1}(\subset\Theta)$ such that $\Theta_{H_0}\cap \Theta_{H_1}=\phi$ and $p_{H_0} = {p(x|\theta); \theta \in \Theta_{H_0}}, p_{H_1} = {p(x|\theta); \theta \in \Theta_{H_1}}$ 
So,
$$H_0 : p \in p_{H_0} \Leftrightarrow H_0 : \theta \in \Theta_{H_0}$$$$H_1 : p \in p_{H_1} \Leftrightarrow H_1 : \theta \in \Theta_{H_1}$$ 
>  **Definition 2:** A hypothesis $H^*$ is called 
>  
>  i. Simple if $H^*$ contains just one parametric point, i.e. $H^*$ specifies the distribution ${p(x|\theta)}$ completely. 
>  
>  ii. Composite if $H^*$ contains more than one parameter point, i.e. $H^*$ cannot specify the distribution ${p(x|\theta)}$ completely.

> **Definition 3** Let $x$ be the observed value of the random variable $X$ with probability model $p(x|\theta);\;\theta$ unknown. Wherever $X=x$ is observed, $p(x|\theta)$ is a function of $\theta$ only and is called the likelihood of getting such a sample. It is simply called the likelihood function and often denoted by $L(\theta)$ or $L(\theta|x)$.

>**Definition 4** ***Test*** is a rule for the acceptance or rejection of the null hypothesis $(H)$ on the basis of an observed value of $X$

>**Definition 5** ***Non-randomized test*** 
>Let $\omega$ be a function of $\mathfrak{x}$ such that
>$$\begin{align} &X\in\omega\Rightarrow\text{The rejection of }H_0 \\ &X\in\mathfrak{x}-\omega\Rightarrow\text{The acceptance of }H_0\end{align}$$ 
>Then $\omega$ is called the critical region or a test for $H_0$ against $H_1$. Test '$\omega$' means a rule determined by $\omega$. Note that $\omega$ does not depend on the random experiment (that is on $X$). So it is called a non-randomized test.

>**Definition 6** ***Randomized test*** 
>This consists of determining a function $\phi(x)$ such that
>(i)$0\leq\phi(x)\leq 1\;\forall x\in\mathfrak{x}$
>(ii)$H_0$ is rejected with a probability $\phi(x)$ when $X=x$ is observed.
>
>Such $\phi(x)$ is called 'Critical function' or 'test function' or simply 'test' for $H_0$ against $H_1$. 
>*e.g, (i) and (ii) $\Rightarrow$ whenever $X=x$ is observed, perform a Bernoullie trial with probability of success $\phi(x)$. If the trial results in a success, reject $H_0$; otherwise $H$ is accepted. Thus $\phi(x)$ represents the probability of rejection of $H_0$*
>
>If $\phi(x)$ is non-randomized with critical region $'\omega'$, then we have 
>$\omega=\left\{x:\phi(x)=1\right\}$
>$x-\omega=\left\{x:\phi(x)=0\right\}$

>**Detailed Study on Non-randomized test**
>If $\omega$ is Non-randomized test then it implies $H_0$ is rejected iff $X\in\omega$. In many cases, we get a statistic $T=T(X)$ such that for some $C$ or $C_1$ and $C_2$,
>$\left[X\in\omega\right]\Leftrightarrow \left[X:T>C\right]$ or $\left[X:T<C\right]$