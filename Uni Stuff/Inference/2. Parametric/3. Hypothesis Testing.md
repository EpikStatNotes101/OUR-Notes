# Hypothesis Testing 

^69f1d3

## Intro 
Suppose we have a random sample from an infinite or a finite population and we try to draw inference regarding the population from such a sample.  Suppose the form of the distribution of the population is $F_\theta$ which is assumed to be known but the parameter $\theta$ is unknown. In many practical problems, we are interested in testing the validity of an assertion about the unknown parameter $\theta$. Some hypothesis is made regarding the parameters and it is tested wheather it is acceptable in the light of the sample observations. For example, suppose we're interested in introducing a high yielding rice variety. We have at our disposal a standard variety having an average yield of $x$ quintal per acre. We want to know whether the average yield of the new variety is higher than $x$. A problem of this type is usually referred to as a problem of testing of hypothesis. 

## Definitions 
Let $\rho = \left\{p(x)\right\}$ be a class of all p.m.f or p.d.f. In testing problem $p(x)$ is unknown, but $\rho$ is known. Our objective is to provide more information about $p(x)$ on the basis of $X = x$. That is to know whether $p(x)\in\rho^*\subset\rho$.

>  **Definition 1:** A hypothesis is a conjecture or assertion about $p(x)$. It is of two types, null hypothesis $(H_0)$ and alternative hypothesis $(H_1)$.

**Null Hypothesis $\mathbf{H_0}$:** 
A hypothesis that is tentatively set up is called null hypothesis. ^d677e1

**Alternative Hypothesis $\mathbf{H_1}$:**
Alternative to $H_0$ is called alternative hypothesis.

We also write $$\left.\begin{matrix}
H_0 : p(x) \in \rho_{H_0} \subset \rho \\ H_1 : p(x) \in \rho_{H_1} \subset \rho
\end{matrix}\right\} \rho_{H_0} \cap \rho_{H_1} = \phi \text{ and } \rho_{H_0} \cup \rho_{H_1} \subseteq \rho$$ 

**Labeling of the distribution** 
Write $\rho = \left\{p(x) = p(x| \theta), \theta \in \Theta \right\}$. Then $'\theta'$ is called the labelling parameter of the distribution and $'\Theta'$ is called the parameter space. 

*Example 1 * 
$X\sim\text{bin}(m,p) \Leftrightarrow X_1, X_2, ...X_m \overset{\text{i.i.d}}{\sim} \text{Bernoulli}(p)$
$\Rightarrow X = \sum_{i=1}^m X_i \sim \text{bin}(m,p)$&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[[2. Discrete Distributions#^29278e|(binomial distribution)]]
$m$ is known, $\theta = p, \Theta = \left[0,1\right]$, outcome space $\mathfrak{x}= \left\{0,1,2,...m\right\}\equiv\left\{0,1\right\}X\left\{0,1\right\}X...X\left\{0,1\right\}$
$$p(x|\theta)=\binom{m}{x}p^x(1-p)^{m-x} \text{ or } p(\tilde{x}|	\theta)=p^{\sum_{i=1}^{m}x_i}(1-p)^{m-\sum_{i=1}^{m}x_i}$$
$$\rho=\left\{\binom{m}{x}p^{x}{(1-p)}^{m-x}, p \in \left[0, 1\right]\right\}$$ is known but $$\binom{m}{x}p^{x}{(1-p)}^{m-x}$$ is unknown if $p$ is unknown.

**Parametric set up**
$p(x)=p(x|\theta); \theta \in \Theta$. Then we can find $\Theta_{H_0}(\subset\Theta)$ and $\Theta_{H_1}(\subset\Theta)$ such that $\Theta_{H_0}\cap \Theta_{H_1}=\phi$ and $p_{H_0} = {p(x|\theta); \theta \in \Theta_{H_0}}, p_{H_1} = {p(x|\theta); \theta \in \Theta_{H_1}}$ 
So,
$$H_0 : p \in p_{H_0} \Leftrightarrow H_0 : \theta \in \Theta_{H_0}$$$$H_1 : p \in p_{H_1} \Leftrightarrow H_1 : \theta \in \Theta_{H_1}$$ 
>  **Definition 2:** A hypothesis $H^*$ is called 
>  
>  i. Simple if $H^*$ contains just one parametric point, i.e. $H^*$ specifies the distribution ${p(x|\theta)}$ completely. 
>  
>  ii. Composite if $H^*$ contains more than one parameter point, i.e. $H^*$ cannot specify the distribution ${p(x|\theta)}$ completely

> **Definition 3** Let $x$ be the observed value of the random variable $X$ with probability model $p(x|\theta);\;\theta$ unknown. Wherever $X=x$ is observed, $p(x|$
