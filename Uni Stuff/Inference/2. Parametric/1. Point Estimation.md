# Point Estimation 
## Intro	
### Random Sampling 
Suppose an experiment results in outcomes $x$ which are values assumed by a r.v. $X$
$$X\sim F$$
This means that $n$ values observed as $x_1,x_2,...x_n$ are assumed by $X$ \[this can be obtained by repeating the experiment under (more or less) identical conditions]. 
$$X_1,X_2,...X_n \overset{\text{i.i.d.}}{\sim} F$$
The set of r.v's $(X_1,X_2,...X_n)$ is called a random sample from the distribution $F$ and the set of values $(x_1,x_2,...x_n)$ is called a realization of the sample. 
### Parameter and Parameter Space
A constant which changes its value from one situation to another is a known parameter, denoted by $\theta$ ($\theta$ might be a vector). The set of all admissible values of a parameter is often called the parameter space, denoted by $\Theta$. 
### Family of Distributions

### Statistic
### Estimator and Estimate
## Sufficient Statistic
Let $\underset{\sim}{X} = (X_1,X_2,...X_n)$ be a random sample from $\left\{ F_{\theta}, \theta \in \Theta \right\}$. A statistic $T(\underset{\sim}{X})$ is said to be sufficient for $\theta$ \[or for the family of distribution $\left\{ F_{\theta}, \theta \in \Theta \right\}$] iff the conditional distribution of $\underset{\sim}{X}$ given $T$ is free from $\theta$
<!--
Include examples
-->
### Fisher-Neyman Factorization Theorem
Let $\underset{\sim}{X} = (X_1,X_2,...X_n)$ be a random sample with c.d.f. $F_\theta\;, \theta\in\Theta$. Let all $X_i$'s be discrete or continous.

Then a statististic $T(\underset{\sim}{X})$ is sufficient for $\theta$ iff the joint p.d.f. or p.m.f. $f(\underset{\sim}{X}, \theta)$, of $X_1,X_2,...X_n$ can be expressed as, 
$$f(\underset{\sim}{x}, \theta)=g\left\{T(\underset{\sim}{x}), \theta \right\}\cdot h(\underset{\sim}{x})$$

Where the first factor $g\left\{T(\underset{\sim}{x}), \theta \right\}$ is a function of $\theta$ and $x$ only through $T(\underset{\sim}{x})$ and for fixed $T(\underset{\sim}{x})$ the second factor $h\left(\underset{\sim}{x}\right)$ is free from $\theta$ and is non-negative.

>**Note** 
>Function being free from $\theta$ implies $\theta$ does not appear in the **functional form** but also the domain of the function does not involve $\theta$
>
>*e.g. the function*
>$$f(x)= \left\{ \begin{align}&1/2, &\theta-1 <x<\theta+1 \\ &0 &\text{o.w.}\end{align}\right.$$
>*is not free of $\theta$*

> #### Corollary 1
> *Let $T(\underset{\sim}{X})$ be a sufficient statistic for $\theta$ and $T'(\underset{\sim}{X}) =\psi\left\{T(\underset{\sim}{X})\right\}$be a one-to-one function of $T$. Then $T'(\underset{\sim}{X})$ is also sufficient for the same parameters $\theta$* 
> 
> ***Proof*** $\because T$ is sufficient for $\theta$, by factorization theorem, $$f(\underset{\sim}{x}, \theta)=g\left\{T(\underset{\sim}{x}), \theta \right\}\cdot h(\underset{\sim}{x})$$
> $\because T'(\underset{\sim}{x})$ is one-to-one
> $$f(\underset{\sim}{x}, \theta)=g\left[\psi^{-1}\left\{T'(\underset{\sim}{x})\right\}, \theta \; \right]\cdot h(\underset{\sim}{x})$$
> Since, the first factor of R.H.S. depends on $\theta$ and $\underset{\sim}{x}$ only through $T'(\underset{\sim}{x})$ and the second factor $h(\underset{\sim}{x})$ is free from $\theta$ and is non-negative. 
> 
> $\therefore$ according to factorizability criterion, we can say that $T'(\underset{\sim}{x})$ is sufficient for the same parameter $\theta$

>**Note** 
>A single sufficient statistic does not always exist. 
>*e.g. Let $X_1,X_2,...X_n$ be a random sample from a population having p.d.f.
>$$f(x,\theta)=\left\{\begin{align}&\frac{1}{\theta}, &k\theta<x<(k+1)\theta, k>0\\ &0 &\text{o.w.}\end{align}\right.$$
>Here no single sufficient statistic exists for $\theta$. In fact $\left\{X_{(1)}, X_{(n)}\right\}$ is sufficient for $\theta$*

>**Note** 
>Not all functions of sufficient statistic are sufficient. 
>*e.g. In random sampling from $N(\mu,\sigma^2), \sigma^2$ being known, $\overset{-}{X}$ is not sufficient for $\mu$*

>**Note** 
>Not all statistic are sufficient

>**Note**
>Let $\underset{\sim}{\theta}=(\theta_1,\theta_2,...\theta_k)$ and $\underset{\sim}{T}=(T_1,T_2,...T_m)$. Further, let $\underset{\sim}{T}$ be a sufficient statistic for $\underset{\sim}{\theta}$. Then we cannot put any restrictions on $m$, i.e. $m\geq k$, the number of parameters involved in the distribution. Even if $m=k$, we cannot say that $T_i\in \underset{\sim}{T}$ is sufficient for $\theta_i \in \underset{\sim}{\theta}$. Instead we could say that $(T_1,T_2,...T_m)$ are jointly sufficient for $(\theta_1,\theta_2,...\theta_k)$


### Distribution Admitting Sufficient Statistic
Let $X_1, X_2,...X_n$ be a random sample from $F_{\underset{\sim}{X}}$ with p.d.f./p.m.f. $f(\underset{\sim}{x}, \theta)$ and $T(\underset{\sim}{x})$ be a sufficient statistic for $\theta$ ($\theta$ is a scalar). 

According to the factorization theorem,
$$\sum_i\log f(x_i,\theta)=\log(T,\theta)+\log h(\underset{\sim}{x})$$
Differentiating w.r.t. $\theta$, we get
$$\sum_i\frac{\delta\log f(x_i,\theta)}{\delta\theta}=\frac{\delta\log g(T,\theta)}{\delta\theta}=G(T,\theta)\text{ (say) }$$..(1)
Then we have
$$\sum_{i=1}^nu_i=G(T)$$
..(2)
Now differentiating (1) and (2) w.r.t. $x_i$, we have
$$\frac{\delta^2\log f(x_i,\theta)}{\delta\theta\delta x_i}=\frac{\delta G}{\delta T}\cdot\frac{\delta T}{\delta x_i}$$
..(3)
$$\frac{\delta u(x_i)}{\delta x_i}=\frac{\delta G(T)}{\delta T}\cdot\frac{\delta T}{\delta x_i}$$
..(4)

(3) and (4) gives us,
$$\frac{\delta^2\log f(x_i,\theta)}{\delta\theta\delta x_i}\Big/\frac{\delta u(x_i)}{\delta x_i}=\frac{\delta G(T, \theta)/\delta T}{\delta G(T)/\delta T}\;\forall i$$
..(5)
Since the RHS of (5) is free from $x_i$, we can write
$$\begin{align}
&\frac{\delta G(T, \theta)}{\delta T}=\frac{\delta G(T)}{\delta T}\cdot\lambda_1(\theta) \\\\
\Rightarrow &G(T, \theta) = G(T)\lambda_1(\theta) + \lambda_2(\theta)\\\\
\Rightarrow &\sum_i \frac{\delta \log f_i(x_i, \theta)}{\delta \theta} =  G(T)\lambda_1(\theta) + \lambda_2(\theta)\\\\
\Rightarrow &\sum_i \log f(x_i, \theta)= G(T)\int\lambda_1(\theta)d\theta + \int\lambda_2(\theta)d\theta + c\left(\underset{\sim}{x}\right)\\\\
\Rightarrow &\prod_i f(x_i, \theta)=A\left(\underset{\sim}{x}\right)e^{\theta_1G(T)+\theta_2}
\end{align}$$
Where $A(\underset{\sim}{x})=$ a function of $\underset{\sim}{x}$ 
$\theta_1=$ a function of $\theta$, and 
$\theta_2=$ another function of $\theta$
$\therefore$ If a distrubution is to have a sufficient statistic for its parameter, it must be of the form
$$f(x,\theta)=e^{B_1(\theta)u(x)+B_2(\theta)+R(x)}$$
This is also known as **Koopman Form**

### Completeness
A family of distributions is said to be complete if
$$\mathbb{E}\left[g\left(X\right)\right]=0 \;\;\forall\theta\in\Theta$$
$$\Rightarrow P\left\{g(x)=0\right\}=1 \;\;\forall\theta\in\Theta$$
A statistic $T$ is said to be complete if family of distributions of $T$ is complete.
### The Sufficiency Principle
A sufficient statistic for parameter $\theta$ is a statistic which if given exhausts all information about $\theta$ and any additional information does not contain any information about $\theta$. 

If $T(\underset{\sim}{X})$ is a sufficient statistic for $\theta$, then any inference about $\theta$ should depend on the sample $\underset{\sim}{X}$ only through the value $T(\underset{\sim}{X})$, that is, if $\underset{\sim}{x}$ and $\underset{\sim}{y}$ are two sample points such that $T(\underset{\sim}{x})=T(\underset{\sim}{y})$, then the inference about $\theta$ should be the same whether $\underset{\sim}{X}=\underset{\sim}{x}$ or $\underset{\sim}{X}=\underset{\sim}{y}$
### Minimal Sufficient Statistic

## Criteria for Choosing Estimator
To determine the true value of the parameter $\theta$ based on a set of expetimentally determined values $x_1,x_2,...x_n$, corresponding to a random sample $X_1,X_2,...X_n$ from $F_\theta$, we choose an estimator $T$ of $\theta$ such that the following condition holds:
$$P\left\{\left|T-\theta\right|<c\right\}\geq P\left\{\left|T'-\theta\right|<c\right\}\forall\theta\in\Theta \text{ and }\forall c$$

Where $T'$ is any rival estimator

Though this is the ideal condition, it's mathametical handling is very difficult and hence we use a simpler condition based on mean square error (m.s.e.). $T$ will be the best in the sense of m.s.e. if
$$E(T-\theta)^2\leq E(T'-\theta)^2\forall \theta$$

where $T'$ is any rival estimator

There exists no $T$ for which the above criteria holds. Hence to sidetrack this we chose an estimator based on the following criterias:
1. unbiasedness
2. minimum-variance
3. consistency
4. efficiency 
## Unbiased Estimator and UMVUE
### Unbiasedness
An estimator $T$ is said to be an unbiased estimator (u.e.) of $\theta$ \[ or $\gamma(\theta)$ ] iff $E(T)=\theta$ \[ or $\gamma(\theta)$ ] $\forall \theta \in \Theta$. Otherwise, it will be called a biased estimator. The quantity $b(\theta, T)=E_\theta(T)-\theta$ is called the **bias**. 

*e.g. Let $X_1,X_2,...X_n$ be a random sample from a population with mean $\mu$ and variance $\sigma^2$. Then $\overset{-}{X}$ and $s^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\overset{-}{X})^2$ are u.e's of $\mu$ and $\sigma^2$, respectively.*

> **Note** 
> 1. Every individual observation is an unbiased estimator of population mean.
> 2. Every partial mean is an unbiased estimator of population mean.
> 3. Every partial sample variance is an unbiased estimator of $\sigma^2$

> **Note** 
> An unbiased estimator may or may not exist.

## Consistent Estimator
## Efficient Estimator
