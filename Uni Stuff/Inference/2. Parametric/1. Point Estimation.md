# Point Estimation 
## Intro	
### Random Sampling 
Suppose an experiment results in outcomes $x$ which are values assumed by a r.v. $X$
$$X\sim F$$
This means that $n$ values observed as $x_1,x_2,...x_n$ are assumed by $X$ \[this can be obtained by repeating the experiment under (more or less) identical conditions]. 
$$X_1,X_2,...X_n \overset{\text{i.i.d.}}{\sim} F$$
The set of r.v's $(X_1,X_2,...X_n)$ is called a random sample from the distribution $F$ and the set of values $(x_1,x_2,...x_n)$ is called a realization of the sample. 
### Parameter and Parameter Space
A constant which changes its value from one situation to another is a known parameter, denoted by $\theta$ ($\theta$ might be a vector). The set of all admissible values of a parameter is often called the parameter space, denoted by $\Theta$. 
### Family of Distributions

### Statistic
### Estimator and Estimate
## Sufficient Statistic
Let $\underset{\sim}{X} = (X_1,X_2,...X_n)$ be a random sample from $\left\{ F_{\theta}, \theta \in \Theta \right\}$. A statistic $T(\underset{\sim}{X})$ is said to be sufficient for $\theta$ \[or for the family of distribution $\left\{ F_{\theta}, \theta \in \Theta \right\}$] iff the conditional distribution of $\underset{\sim}{X}$ given $T$ is free from $\theta$
<!--
Include examples
-->
### Fisher-Neyman Factorization Theorem
Let $\underset{\sim}{X} = (X_1,X_2,...X_n)$ be a random sample with c.d.f. $F_\theta\;, \theta\in\Theta$. Let all $X_i$'s be discrete or continous.

Then a statististic $T(\underset{\sim}{X})$ is sufficient for $\theta$ iff the joint p.d.f. or p.m.f. $f(\underset{\sim}{X}, \theta)$, of $X_1,X_2,...X_n$ can be expressed as, 
$$f(\underset{\sim}{x}, \theta)=g\left\{T(\underset{\sim}{x}), \theta \right\}\cdot h(\underset{\sim}{x})$$

Where the first factor $g\left\{T(\underset{\sim}{x}), \theta \right\}$ is a function of $\theta$ and $x$ only through $T(\underset{\sim}{x})$ and for fixed $T(\underset{\sim}{x})$ the second factor $h\left(\underset{\sim}{x}\right)$ is free from $\theta$ and is non-negative.

>**Note 1** 
>Function being free from $\theta$ implies $\theta$ does not appear in the **functional form** but also the domain of the function does not involve $\theta$
>
>*e.g. the function*
>$$f(x)= \left\{ \begin{align}&1/2, &\theta-1 <x<\theta+1 \\ &0 &\text{o.w.}\end{align}\right.$$
>*is not free of $\theta$*

> #### Corollary 1
> *Let $T(\underset{\sim}{X})$ be a sufficient statistic for $\theta$ and $T'(\underset{\sim}{X}) =\psi\left\{T(\underset{\sim}{X})\right\}$be a one-to-one function of $T$. Then $T'(\underset{\sim}{X})$ is also sufficient for the same parameters $\theta$* 
> 
> ***Proof*** $\because T$ is sufficient for $\theta$, by factorization theorem, $$f(\underset{\sim}{x}, \theta)=g\left\{T(\underset{\sim}{x}), \theta \right\}\cdot h(\underset{\sim}{x})$$
> $\because T'(\underset{\sim}{x})$ is one-to-one
> $$f(\underset{\sim}{x}, \theta)=g\left[\psi^{-1}\left\{T'(\underset{\sim}{x})\right\}, \theta \; \right]\cdot h(\underset{\sim}{x})$$
> Since, the first factor of R.H.S. depends on $\theta$ and $\underset{\sim}{x}$ only through $T'(\underset{\sim}{x})$ and the second factor $h(\underset{\sim}{x})$ is free from $\theta$ and is non-negative. 
> 
> $\therefore$ according to factorizability criterion, we can say that $T'(\underset{\sim}{x})$ is sufficient for the same parameter $\theta$
### Completeness
### The Sufficiency Principle
### Minimal Sufficient 
## Unbiased Estimator and UMVUE
To determine the true value of the parameter $\theta$ based on a set of expetimentally determined values $x_1,x_2,...x_n$, corresponding to a random sample $X_1,X_2,...X_n$ from $F_\theta$, we choose an estimator $T$ of $\theta$ such that the following condition holds:
$$P\left\{\left|T-\theta\right|<c\right\}\geq P\left\{\left|T'-\theta\right|<c\right\}\forall\theta\in\Theta \text{ and }\forall c$$
### Unbiasedness
## Consistent Estimator
## Efficient Estimator
