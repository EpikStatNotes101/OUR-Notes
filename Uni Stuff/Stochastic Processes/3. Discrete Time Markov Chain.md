# Discrete Time Markov Chain
Let $\left\{X_n, n\geq 0 \right\}$ be a stochastic process taking values in a state space $S$ that has $N$ states. such a stochastic process is a Markov processes if it satisfies a following property : 

$$P(X_{n+1}=k_{n+1}|X_n = k_{n}, X_{n-1} = k_{n-1},...X_{1} = k_{1})=P(X_{n+1}=k_{n+1}|X_n = k_{n})$$

For a markov process, the *future state* only depends on the *present state* and not on the *past states*. 

If the state space of a Markov process is discrete, it's called a **Markov Chain**.

To understand the behaviour of this process, we will need to calculate probabilities like, 
$$P\left[X_0 = i_0, X_1 = i_1, ..., X_n = i_n\right]$$ ..(1)

$\because P(A,B)=P(A)\cdot P(B|A)$, this can be computed by multiplying conditional probabilities as follows. 

$$= P(X_0=i_0)\cdot P(X_1=i_1|X_0=i_0) \cdot P(X_2=i_2|X_1 = i_1, X_0=i_0)...$$ $$P(X_n=i_n|X_{n-1} = i_{n-1}, X_{n-2} = i_{n-2}, ..., X_0=i_0)$$ ..(2)

From the markovian property, 
$$= P(X_0=i_0)\cdot P(X_1=i_1|X_0=i_0) \cdot P(X_2=i_2|X_1 = i_1)...P(X_n=i_n|X_{n-1} = i_{n-1})$$ ..(3)

## State Transition Probabilities
For a discrete time Markov Chain $\left\{X_n: n=1,2,...\right\}$ with discrete state space $S=\left\{0, 1, 2, ...\right\}$ where this set may be finite or infinite, if $X_n=i$ then the Markov Chain is said to be in state $i$ at time $n$(or the $n^{\text{th}}$ step)

### One Step Transition Probability

A discrete time Markov Chain $\left\{X_n: n=1,2,...\right\}$ is characterized by 

$$P\left[X_{n+1}=i_{n+1}|X_n=i_n,...,X_0=i_0\right]=P\left[X_{n+1}=i_{n+1}|X_n=i_n\right]$$

Where $P\left[X_{n+1}=j|X_n=i\right]$ is called one step transition probability

If $P\left[X_{n+1}=j|X_n=i\right]$ is independent of $n$ then the Markov Chain is said to possess stationary transition probabilities and the process is reffered to as a homogeneous Markov Chain. Otherwise the process is called a non-homogeneous Markov Chain. 

### Transition Probability Matrix
The matrix called the **state transition matrix (t.p.m)** or **transition probability matrix** is usually denoted by $P$. 

Let $\left\{X_n: n=1,2,...\right\}$ be a homogenous Markov Chain with a discrete finite state space $S=\left\{0, 1, 2, ...,m\right\}$ then 
$$p_{ij}=P\left[X_{n+1}=j|X_n=i\right]
\;\;\; i\geq 0, j\geq 0$$ 
regardless of the value of $n$.

A t.p.m of  $\left\{X_n\right\}$ is defined by 

$$P=\left[ p_{ij} \right]=
\left[\begin{matrix} 
&p_{11} &p_{12} &... &p_{1m} &\\ 
&p_{21} &p_{22} &... &p_{2m} &\\ 
&p_{31} &p_{32} &... &p_{3m} &\\
&\vdots &\ddots&&&\\
&p_{m1} &p_{m2} &... &p_{mm} &\\

\end{matrix}\right]$$

Where $$p_{ij}\geq 0$$ and $$\sum_{j=1}^m p_{ij} = 1, \;\;\;\; i=1,2,...,m$$

### State Transition Diagram
A Markov Chain is usually shown by a state transition diagram. Consider a Markov Chain with three possible states $S = \left\{1, 2, 3\right\}$ and the following transition probabilities 

$$P=
\left[\begin{matrix} 
&\frac{1}{4}	&\frac{1}{2}	&\frac{1}{4} &\\ 
&\frac{1}{3}	&0				&\frac{2}{3} &\\ 
&\frac{1}{2}	&0				&\frac{1}{2} &\\

\end{matrix}\right]
=
\left[\begin{matrix} 
&p_{11} &p_{12} &p_{13} &\\ 
&p_{21} &p_{22} &p_{23} &\\ 
&p_{31} &p_{32} &p_{33} &\\

\end{matrix}\right]$$

Which satisfies the two criterias, i.e. $$p_{ij}\geq 0$$ and $$\sum_{j=1}^3 p_{ij} = 1, \;\;\;\; i=1,2,3$$

The figure below shows the state transition diagram for this Markov Chain

![[Uni Stuff/Stochastic Processes/Diagrams/Markov1.png|250]]

### $n$-step Transition Probability Matrix 
Consider a Markov Chain $\left\{X_n: n=0,1,2,...\right\}$ if $X_0=i$ then $X_1=j$ with probability $p_{ij}$ is the probability of going from state $i$ to state $j$ in one step. 

Now suppose we're interested in finding the probability of going from state $i$ to state $j$ in two steps, i.e. 