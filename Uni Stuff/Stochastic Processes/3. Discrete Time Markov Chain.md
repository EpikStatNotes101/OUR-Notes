# Discrete Time Markov Chain
Let $\left\{X_n, n\geq 0 \right\}$ be a stochastic process taking values in a state space $S$ that has $N$ states. such a stochastic process is a Markov processes if it satisfies a following property : 

$$P(X_{n+1}=k_{n+1}|X_n = k_{n}, X_{n-1} = k_{n-1},...X_{1} = k_{1})=P(X_{n+1}=k_{n+1}|X_n = k_{n})$$

For a markov process, the *future state* only depends on the *present state* and not on the *past states*. 

If the state space of a Markov process is discrete, it's called a **Markov Chain**.

To understand the behaviour of this process, we will need to calculate probabilities like, 
$$P\left[X_0 = i_0, X_1 = i_1, ..., X_n = i_n\right]$$ ..(1)

$\because P(A,B)=P(A)\cdot P(B|A)$, this can be computed by multiplying conditional probabilities as follows. 

$$= P(X_0=i_0)\cdot P(X_1=i_1|X_0=i_0) \cdot P(X_2=i_2|X_1 = i_1, X_0=i_0)...$$ $$P(X_n=i_n|X_{n-1} = i_{n-1}, X_{n-2} = i_{n-2}, ..., X_0=i_0)$$ ..(2)

From the markovian property, 
$$= P(X_0=i_0)\cdot P(X_1=i_1|X_0=i_0) \cdot P(X_2=i_2|X_1 = i_1)...P(X_n=i_n|X_{n-1} = i_{n-1})$$ ..(3)

## State Transition Probabilities
For a discrete time Markov Chain $\left\{X_n: n=1,2,...\right\}$ with discrete state space $S=\left\{0, 1, 2, ...\right\}$ where this set may be finite or infinite, if $X_n=i$ then the Markov Chain is said to be in state $i$ at time $n$(or the $n^{\text{th}}$ step)

### One Step Transition Probability

A discrete time Markov Chain $\left\{X_n: n=1,2,...\right\}$ is characterized by 

$$P\left[X_{n+1}=i_{n+1}|X_n=i_n,...,X_0=i_0\right]=P\left[X_{n+1}=i_{n+1}|X_n=i_n\right]$$

Where $P\left[X_{n+1}=j|X_n=i\right]$ is called one step transition probability

If $P\left[X_{n+1}=j|X_n=i\right]$ is independent of $n$ then the Markov Chain is said to possess stationary transition probabilities and the process is reffered to as a homogeneous Markov Chain. Otherwise the process is called a non-homogeneous Markov Chain. 

### Transition Probability Matrix
The matrix called the **state transition matrix (t.p.m)** or **transition probability matrix** is usually denoted by $P$. 

Let $\left\{X_n: n=1,2,...\right\}$ be a homogenous Markov Chain with a discrete finite state space $S=\left\{0, 1, 2, ...,m\right\}$ then 
$$p_{ij}=P\left[X_{n+1}=j|X_n=i\right]
\;\;\; i\geq 0, j\geq 0$$ 
regardless of the value of $n$.

A t.p.m of  $\left\{X_n\right\}$ is defined by 

$$P=\left[ p_{ij} \right]=
\left[\begin{matrix} 
&p_{11} &p_{12} &... &p_{1m} &\\ 
&p_{21} &p_{22} &... &p_{2m} &\\ 
&p_{31} &p_{32} &... &p_{3m} &\\
&\vdots &\ddots&&&\\
&p_{m1} &p_{m2} &... &p_{mm} &\\

\end{matrix}\right]$$

Where $$p_{ij}\geq 0$$ and $$\sum_{j=1}^m p_{ij} = 1, \;\;\;\; i=1,2,...,m$$

### State Transition Diagram
A Markov Chain is usually shown by a state transition diagram. Consider a Markov Chain with three possible states $S = \left\{1, 2, 3\right\}$ and the following transition probabilities 

$$P=
\left[\begin{matrix} 
&\frac{1}{4}	&\frac{1}{2}	&\frac{1}{4} &\\ 
&\frac{1}{3}	&0				&\frac{2}{3} &\\ 
&\frac{1}{2}	&0				&\frac{1}{2} &\\

\end{matrix}\right]
=
\left[\begin{matrix} 
&p_{11} &p_{12} &p_{13} &\\ 
&p_{21} &p_{22} &p_{23} &\\ 
&p_{31} &p_{32} &p_{33} &\\

\end{matrix}\right]$$

Which satisfies the two criterias, i.e. $$p_{ij}\geq 0$$ and $$\sum_{j=1}^3 p_{ij} = 1, \;\;\;\; i=1,2,3$$

The figure below shows the state transition diagram for this Markov Chain

![[Uni Stuff/Stochastic Processes/Diagrams/Markov1.png|250]]

### $n$-step Transition Probability
Consider a Markov Chain $\left\{X_n: n=0,1,2,...\right\}$ if $X_0=i$ then $X_1=j$ with probability $p_{ij}$ is the probability of going from state $i$ to state $j$ in one step. 

Now suppose we're interested in finding the probability of going from state $i$ to state $j$ in two steps, i.e. 

$$p_{ij}^{(2)}=P(X_2=j|X_0=i)$$
$$p_{ij}^{(2)}=P(X_{n+2}=j|X_n=i)$$

We can find the probability by applying the law of total probability $X_1$ can take one of the possible values of $S$

$$p_{ij}^{(2)}=P(X_2=j|X_0=i)$$
$$=\sum_{k \in S}P(X_2=j|X_1=k, X_0=i)\cdot P(X_1=k|X_0=i)$$ 
\[by law of total probability\]
$$=\sum_{k \in S}P(X_2=j|X_1=k)\cdot P(X_1=k|X_0=i)$$ \[by markovian property\] 
$$=\sum_{k \in S}p_{kj} \cdot p_{ik}$$
$$=\sum_{k \in S}p_{ik} \cdot p_{kj}$$

$$\therefore p_{ij}^{(2)}=P(X_2=j|X_0=i)=\sum_{k \in S}p_{ik} \cdot p_{kj}$$

Which means that in order to get to state $j$ from $i$, we need to pass through some intermediate state $k$. 

$$i\longrightarrow k\longrightarrow j$$

### $n$-step Transition Probability Matrix

We can define the **two-step transition matrix** as 
$$P^{(2)}=
\left[\begin{matrix} 
&p^{(2)}_{11} &p^{(2)}_{12} &... &p^{(2)}_{1m} &\\ 
&p^{(2)}_{21} &p^{(2)}_{22} &... &p^{(2)}_{2m} &\\ 
&p^{(2)}_{31} &p^{(2)}_{32} &... &p^{(2)}_{3m} &\\
&\vdots &\ddots&&&\\
&p^{(2)}_{m1} &p^{(2)}_{m2} &... &p^{(2)}_{mm} &\\

\end{matrix}\right]
$$

We conclude that two-step transition matrix can be obtained by squaring the state transition matrix. 
$$P^{(2)}= P\cdot P = P^2$$

Similarly, 
$$P^{(3)}= P\cdot P^2 = P\cdot P^{(2)}$$

Generally we can define the $n$-step transition probability $p_{ij}^{(n)}$ as 

$$p_{ij}^{(n)}=P(X_n=j|X_0=i), \;\;\;\;\;\; i=0,1,2,...$$

In order to get from state $i$ to state $j$, we need to pass through $n-1$ intermediate states $k_1, k_2, ..., k_{n-1}$

$$i\longrightarrow k_1\longrightarrow k_2 \longrightarrow ... \longrightarrow k_{n-1} \longrightarrow j$$

The **$n$-step transition matrix** is defined as follows

$$P^{(n)}=
\left[\begin{matrix} 
&p^{(n)}_{11} &p^{(n)}_{12} &... &p^{(n)}_{1r} &\\ 
&p^{(n)}_{21} &p^{(n)}_{22} &... &p^{(n)}_{2r} &\\ 
&p^{(n)}_{31} &p^{(n)}_{32} &... &p^{(n)}_{3r} &\\
&\vdots &\ddots&&&\\
&p^{(n)}_{r1} &p^{(n)}_{r2} &... &p^{(n)}_{rr} &\\

\end{matrix}\right]
$$

$$P^{(n)} = P^n$$


Let $m$ and $n$ be two positive integers and assume $X_0=i$. In order to get to state $j$ in $(m+n)$ steps, the chain will be at some intermediate state $k$ after $m$ steps. 

To obtain $$p_{ij}^{(m+n)} = P\left[X_{n+m}=j|X_0=i\right]$$
$$=\sum_{k\in S}p^{(n)}_{ik} \cdot p^{(m)}_{kj}$$

This equation is called the **Chapmanâ€“Kolmogorov Equation**

## Probability distribution of $X_n, n\geq 0$
Consider a Markov Chain $\left\{X_n: n=0,1,2,...\right\}$. Suppose we know the probability distribution of $X_0$. 

Define the row vector $\pi^{(0)}$ as 

$$\pi^{(0)}= \left[\begin{matrix}
&P(X_0=1) &P(X_0=2) &... &P(X_0=r) &
\end{matrix}\right]$$

Now, we can obtain the probability distribution of $X_1, X_2, ...$. 

Using the law of total probability, for anu $j \in S$, we can write

$$P(X_1=j)=\sum_{k=1}^r P(X_1=j|X_0=k)\cdot P(X_0=k)$$
$$=\sum_{k=1}^r p_{kj}\cdot P(X_0=k)$$

$$\pi^{(n)}=\left[\begin{matrix}
&P(X_n=1) &P(X_n=2) &... &P(X_n=r) &
\end{matrix}\right]$$

Given the state transition matrix $P$, we can rewrite the above results in the form of matrix multiplication

$$\pi^{(1)}=\pi^{(0)}\cdot P$$
$$\pi^{(2)}=\pi^{(1)}\cdot P$$
$$\vdots$$
$$\pi^{(n)}=\pi^{(n-1)}\cdot P$$

or

$$\pi^{(n)}=\pi^{(0)}\cdot P^n$$

### Stationary Distribution of a Markov Chain 
Given a t.p.m $P$ of a markov chain $\left\{X_n: n=0,1,2,...\right\}$, if there exists a probability vector $\hat{p}$ which satisfies 
$$\hat{p}\cdot P = \hat{p}$$ ..(1)

Then $\hat{p}$ is called a stationary distribution for the given markov chain.

The stationary distribution vector represents the distribution of all states over an infinitely long run. 

## Types of States 
States can be categorized into the following types

### 1. Accessible States 
A state $j$ is said to be **accessible** from a state $i$ if 
$$p_{ij}^{(n)} > 0 \;\;\;\text{  for some  } n$$
We denote it as $i\longrightarrow j$.

### 2. Communicative States 
Two states $i$ and $j$ are said to be **communicative** if $i$ is accessible from $j$ and vice versa. We denote such states as $i\longleftrightarrow j$. 

In other words, $$i\longrightarrow j \;\;\&\;\; j\longrightarrow i \Rightarrow i\longleftrightarrow j$$

### Irreducible Markov Chain 
A Markov Chain is said to be irreducible if all states communicate with each other.

### Properties of an irreducible Markov Chain
1.  Every state communicates with each other. $$i\longleftrightarrow j \;\;\;\forall i, j$$
2.  $i\longleftrightarrow j$ implies $j\longleftrightarrow i$.
3.  $i\longleftrightarrow j$ and $j\longleftrightarrow k$ together implies $i\longleftrightarrow k$.

#### Regular Transition Matrix
A transition matrix $P$ is said to be **regular** if there is some $n$ for which $P^{(n)}$ contains all positive non-zero elements. 

- If the transition matrix is not irreducible then it is *not regular*. 
- If the transition matrix is irreducible and at least one entry of the main diagonal is non-zero, then it is *regular*
- If all entries of the main diagonal are zero, but there exists some $n$ for which $P^{(n)}$ contains all positive entries, then it is *regular*


